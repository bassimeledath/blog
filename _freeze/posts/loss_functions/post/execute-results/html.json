{
  "hash": "d0b41458c48ba2c0de64b28445984840",
  "result": {
    "markdown": "---\ntitle: 'Calculus Made Pythonic: Loss Functions and Gradient Descent Explained'\ndate: last-modified\ncategories: ['Machine Learning', 'Calculus']\ndescription: 'This is a python post'\nexecute: \n  message: false\n  warning: false\neditor_options: \n  chunk_output_type: console\n---\n\n# Motivation\nThe transition from calculus to machine learning can seem intimidating for many. During my first machine learning course, I struggled with understanding how the derivative rules, which I had merely memorized in calculus, could be relevant to training machine learning models. The varied and sometimes inconsistent mathematical notations and language in textbooks further muddled my comprehension.\n\nHowever, over the subsequent years, I revisited calculus, but this time through the lens of coding. Suddenly, everything began to make sense. In this article, I aim to simplify this complex transition for newcomers to machine learning. \n\nTO-DO\nBy the end of this post, I am confident you will be able to:\n\n\nLet us now dive into the world of calculus by first setting up a problem.\n\n## Help I have a math exam!\n\nLet's say you have a math exam tomorrow, and you want to know what optimal number of hours you should study to maximize your grade. You know that the more you study, the higher your grade will be...but only up to a point. If you study too much, the more tired you will be, and the less you will retain. In other words, there is some non-linear relationship between the number of hours you study and your grade.\n\nSo you read a research paper that studied many math students in the past and identified the following relationship between the number of points on the exam students lose as a function of the number of hours studied.\n\n$$\nL(x) = x^2 - 12x + 36\n$$\n\nHere `x` represent the number of hours spent studying for the math exam. The function `L(x)` represents the number of points lost on the exam as a function of the number of hours studied. Henceforth, I will be referring to this function as the **loss function** not only because it represents the number of points lost but also because it is the standard term used in machine learning to describe a function we'd like to minimize.\n\nFor example, if you study for 0 hours, you will lose 36 points according to the function. If you study for 3 hours you lose fewer points. But if you study for too long (say 11 hours), you will lose 25 points.\n\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef L_math(x):\n    return x**2 - 12*x + 36\n\nprint(f\"0 hours study: {L_math(0)} points lost\")\nprint(f\"3 hour study: {L_math(3)} points lost\")\nprint(f\"11 hour study: {L_math(11)} points lost\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0 hours study: 36 points lost\n3 hour study: 9 points lost\n11 hour study: 25 points lost\n```\n:::\n:::\n\n\n### The naive approach\n\nSo what's the optimal number of hours? Well the naive approach would be to simply plug a bunch of numbers into the function and see which one gives you the lowest number of points lost. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\nx = np.random.choice(np.arange(0, 13), 3, replace=False,)\n\ndf = pd.DataFrame({'x': x, 'L_math': L_math(x)})\nax = df.plot(x='x', y='L_math', kind='scatter', marker='x', color='red', s=75)\n\n# Set the x-axis label\nax.set_xlabel(\"Hours Studied\", labelpad=20, weight='bold', size=12)\n\n# Set the y-axis label\nax.set_ylabel(\"Points Lost\", labelpad=20, weight='bold', size=12)\n\n# Set y-axis range\nax.set_ylim(0, 40)\n\n# Remove top and right borders\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Annotate each point with its y-value\nfor _, row in df.iterrows():\n    ax.annotate(f\"{row['L_math']:.2f}\", (row['x'], row['L_math']), textcoords=\"offset points\", xytext=(0,5), ha='center')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](post_files/figure-html/cell-4-output-1.png){width=609 height=458}\n:::\n:::\n\n\nBut as you can imagine, this is a very inefficient approach as we'd have to keep guessing for a long time. Instead, we can use calculus to help us get to the answer much faster.\n\n## Calculus to the rescue\n\nLet's start at a random point, say x=1 representing one hour of study.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nprint(f\"1 hour study: {L_math(1)} points lost\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1 hour study: 25 points lost\n```\n:::\n:::\n\n\nNow the question is if the optimal study time is *longer* or *shorter* than 1 hour? Do we move to the *right* or to the *left* along the x-axis? In other words, what direction can we go in order to minimize the number of points lost.\n\nHow about we experiment by slightly nudging the x-value to the right by 0.01 hours and see what happens to the number of points lost?\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nx = 1\nadditional_hours = 0.01\n\nprint(round(L_math(x + additional_hours) - L_math(x), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-0.1\n```\n:::\n:::\n\n\nWe observe that we lose fewer points (approximately 0.1 points) by studying for an additional 0.01 hours. So we should continue to study for longer.\n\nBut that's just the absolute difference in points lost - that number could be really tiny if we're comparing two points very close to each other. Instead, what we really want to know is the *rate of change* in points lost around x=1 (between x=1 and x=1.01). So we divide the absolute difference by the number of additional hours studied.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nprint(round((L_math(x + additional_hours) - L_math(x)) / additional_hours, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-9.99\n```\n:::\n:::\n\n\nAnd we get -9.99 which indicates the function is decreasing at that point, and for a tiny increase in x (by 0.01 hours), the function decreases by about 9.99 times that tiny increase. \n\nWhat we've just done is calculate something akin to the *derivative* of the loss function with respect to number of hours studied at x=1. The derivative of a function at a particular point represents the rate of change of the function at that specific point. In simpler words, it tells us how much the function changes when we nudge the input value by a tiny amount.\n\nHowever, what we computed is what one might call a \"crude\" or \"numerical\" derivative, which is an approximation to the true derivative. This is because we are using a fixed interval (0.01 hours) to measure the difference. The true derivative, in a mathematical sense, refines this concept by letting that interval become infinitesimally small, that is, approach zero. In formal calculus terms, this is represented as:\n\n$$\n\\frac{dL_{\\text{math}}(x)}{dx} = \\lim_{\\Delta x \\to 0} \\frac{L_{\\text{math}}(x + \\Delta x) - L_{\\text{math}}(x)}{\\Delta x}\n$$\n\nThe left side of the equation, $\\frac{dL_{\\text{math}}}{dx}$ is the notation for the derivative of $L_{\\text{math}}$ with respect to $x$. The right side captures the idea of the rate of change, with the added twist that we're considering the change as $\\Delta x$\ngets smaller and smaller, effectively zero.\n\nGreat! So we know that we should study for longer. But how much longer? When do we know when to stop studying?  This is where we can use *gradient descent*. \n\n### Getting to the optimal number of hours\n\nOkay so moving from x=1 to x=1.01 hours of study was a good move. How about we continue to move in that direction? Let's move from x=1.01 to x=1.02 hours of study.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nx = 1.01\nadditional_hours = 0.01\n\nprint(round((L_math(x + additional_hours) - L_math(x)) / additional_hours, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-9.97\n```\n:::\n:::\n\n\nSeeing that the rate of change remains negative, it's evident that studying longer will continue to decrease our loss. The negative rate of change is essentially telling us, \"Keep going! Studying more is beneficial.\"\n\nWhen do we stop? Well, we stop when the rate of change becomes zero or positive, which means every step therafter would result in more points lost. Let's keep going until that happens.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nx = 1.01\nadditional_hours = 0.01\nnum_steps = 0\nwhile (L_math(x + additional_hours) - L_math(x)) / additional_hours < 0:\n    x += additional_hours\n    num_steps += 1\n\nprint(f\"Optimal number of hours of study: {round(x,4)}\")\nprint(f\"Points lost: {round(L_math(x),4)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimal number of hours of study: 6.0\nPoints lost: -0.0\n```\n:::\n:::\n\n\nWe see that the optimal number of hours of study is 6.0, and the number of points lost is 0.0. This is the minimum value of the loss function.\n\nBut...\n\nImagine that apart from the math exam, students also had to prepare for an English exam. Now, our task becomes a bit more challenging. We have two variables, `x` representing the number of hours studied for the math exam and `y` representing the number of hours studied for the English exam.\n\nFor the English exam, research indicated the loss function to be:\n\n$$\nL(y) = 2y^2 - 20y + 50 \n$$\n\nThe combined loss function, which signifies the total points lost across both exams, can be represented as:\n$$\nL_{\\text{combined}}(x,y) = L_{\\text{math}}(x) + L_{\\text{english}}(y)\n$$\n\nNow, we're not just optimizing for a single variable, but two. The optimal number of hours to study for both exams would be the point where this combined loss function is minimized.\n\nTo do this, we can't just take a single derivative like before, since we now have two variables to consider. We need something called a partial derivative.\n\nA partial derivative tells us how a function changes when we vary one variable, keeping the other variables fixed. For our combined loss function, we need to compute two partial derivatives:\n\n1. The rate of change of the combined loss with respect to `x`, while keeping `y` constant.\n2. The rate of change of the combined loss with respect to `y`, while keeping `x` constant.\n\nMathematically, these are represented as:\n\n$$\n\\partial L_{\\text{combined}} / \\partial x \\text{ and } \\partial L_{\\text{combined}} / \\partial y\n$$\n\nThese partial derivatives will give us the gradient or slope in the direction of `x` and `y` respectively. If you recall our earlier discussion, a negative gradient indicates we should increase our study hours in that subject, and a positive gradient suggests we should decrease.\n\nSo how do we apply gradient descent with two variables? It's quite similar to how we did it last time but we now have two directions we can go in - the x and y directions. So at each step:\n\n1. We compute the gradient in the `x` direction and the `y` direction.\n2. We update `x` and `y` based on their gradients and a learning rate.\n\nThe process continues until both gradients are close to zero, indicating we're near the minimum of our combined loss function.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndef L_math(x):\n    return x**2 - 12*x + 36\n\ndef L_english(y):\n    return 2*y**2 - 20*y + 50\n\ndef L_combined(x, y):\n    return L_math(x) + L_english(y)\n\ndef partial_derivative(variable, x, y, step=0.01):\n    current_value = L_combined(x, y)\n    \n    # Adjust x or y based on which variable's derivative we're calculating.\n    if variable == 'x':\n        # Increase x slightly and calculate the new value.\n        new_value = L_combined(x + step, y)\n    elif variable == 'y':\n        # Increase y slightly and calculate the new value.\n        new_value = L_combined(x, y + step)\n\n    gradient = (new_value - current_value) / step\n    return gradient\n\n# Initialize the study hours for math and English.\nx, y = 1, 1\nlearning_rate = 0.01\nnum_steps = 0\n\n# Continue until both gradients suggest we shouldn't increase the study hours.\nwhile True:\n    x_gradient = partial_derivative('x', x, y, learning_rate)\n    y_gradient = partial_derivative('y', x, y, learning_rate)\n    \n    # Stop if both gradients are positive.\n    if x_gradient >= 0 and y_gradient >= 0:\n        break\n    \n    # Update the study hours for each subject by moving in the opposite direction of the gradient.\n    x -= learning_rate * x_gradient\n    y -= learning_rate * y_gradient\n    num_steps += 1\n\n# Display the optimal study hours and the total points lost at that minimum.\nprint(f\"Optimal study hours for math: {round(x, 2)}\")\nprint(f\"Optimal study hours for English: {round(y, 2)}\")\nprint(f\"Total points lost: {round(L_combined(x, y), 2)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimal study hours for math: 6.0\nOptimal study hours for English: 4.99\nTotal points lost: 0.0\n```\n:::\n:::\n\n\nWe see that the optimal number of hours of study for math is 6.0 and for English is 5.0. The total number of points lost is 0.0.\n\n\n\n###### DRAFT\nWe see that the numbers points decreases slightly. \n\nchange in points lost over change in hours studied\n\nBut this iterative, manual adjustment to x is both tedious and slow. What if we could automate it, and more importantly, optimize it?\n\nEnter, gradient descent. By repetitively adjusting x in the direction of the steepest decrease of our loss function, we aim to find the value of x that minimizes our loss in the shortest amount of iterations. This method is called gradient descent.\n\n",
    "supporting": [
      "post_files"
    ],
    "filters": [],
    "includes": {}
  }
}